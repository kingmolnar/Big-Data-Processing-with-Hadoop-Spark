{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Data Frames\n",
    "\n",
    "Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to perform extra optimizations.\n",
    "\n",
    "There are several ways to interact with Spark SQL including SQL, the DataFrames API and the Datasets API.\n",
    "When computing a result the same execution engine is used, independent of which API/language you are using to express the computation. This unification means that developers can easily switch back and forth between the various APIs based on which provides the most natural way to express a given transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-21T11:28:15.750651",
     "start_time": "2017-10-21T11:28:14.410054"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-21T11:38:44.752076",
     "start_time": "2017-10-21T11:28:24.142001"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launched Spark version 1.6.1 with ID application_1508160140652_0078\n"
     ]
    }
   ],
   "source": [
    "# %load ../01_Distributed_Computing_HDFS_Distributed_Data_Sets/pyspark_init_arc.py\n",
    "#\n",
    "# This configuration works for Spark on arc.insight.gsu.edu\n",
    "#\n",
    "import os, sys\n",
    "# set OS environment variable\n",
    "os.environ[\"SPARK_HOME\"] = '/usr/hdp/2.4.2.0-258/spark'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-csv_2.11:1.2.0 pyspark-shell'\n",
    "\n",
    "# add Spark library to Python\n",
    "sys.path.insert(0, os.path.join(os.environ[\"SPARK_HOME\"], 'python'))\n",
    "\n",
    "# import package\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext, SparkConf\n",
    "\n",
    "import atexit\n",
    "def stop_my_spark():\n",
    "    sc.stop()\n",
    "    del(sc)\n",
    "\n",
    "# Register exit    \n",
    "atexit.register(stop_my_spark)\n",
    "\n",
    "# Configure and start Spark ... but only once.\n",
    "if not 'sc' in globals():\n",
    "    conf = SparkConf()\n",
    "    conf.setAppName('MyFirstSpark') ## you may want to change this\n",
    "    conf.setMaster('yarn-client')\n",
    "    ##conf.set('spark.ui.port', '%d'%(52000+np.int(np.random.rand(1)*10000)))\n",
    "    sc = SparkContext(conf=conf)\n",
    "    print \"Launched Spark version %s with ID %s\" % (sc.version, sc.applicationId)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-21T11:38:44.760839",
     "start_time": "2017-10-21T11:38:44.754737"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://arc.insight.gsu.edu:8088/cluster/app/application_1508160140652_0078\n"
     ]
    }
   ],
   "source": [
    " print \"http://arc.insight.gsu.edu:8088/cluster/app/%s\"% (sc.applicationId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T17:49:39.261889",
     "start_time": "2017-10-20T17:49:39.256856"
    }
   },
   "source": [
    "## Add SQL Context and a couple of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-21T08:47:37.406548",
     "start_time": "2017-10-21T08:47:37.397001"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, Row, DataFrame\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T17:55:43.336507",
     "start_time": "2017-10-20T17:55:36.346730"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_df = sqlCtx.read.json('/data/yelp/user')\n",
    "user_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many records?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T17:56:19.448928",
     "start_time": "2017-10-20T17:56:15.587524"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T17:57:21.070264",
     "start_time": "2017-10-20T17:57:20.485665"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlCtx.read.json()\n",
    "user_df.select('name', 'average_stars', 'compliments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T18:21:25.794089",
     "start_time": "2017-10-20T18:21:25.252019"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_df.registerTempTable('users')\n",
    "sqlCtx.sql(\"SELECT name, average_stars, compliments FROM users WHERE average_stars > 4\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T20:16:56.318407",
     "start_time": "2017-10-20T20:16:41.708709"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "review_df = sqlCtx.read.json('/data/yelp/review')\n",
    "review_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T20:17:10.076563",
     "start_time": "2017-10-20T20:17:10.072621"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_df.registerTempTable('reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T20:30:05.135425",
     "start_time": "2017-10-20T20:30:04.892005"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jnt_df = sqlCtx.sql(\"\"\"\n",
    "SELECT business_id, AVG(stars) AS Mstars, VARIANCE(stars) AS Vstars, COUNT(*) AS n FROM users\n",
    "JOIN reviews \n",
    "ON users.user_id=reviews.user_id\n",
    "GROUP BY business_id\n",
    "HAVING COUNT(*)>20\n",
    "\"\"\")\n",
    "jnt_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T20:30:59.331201",
     "start_time": "2017-10-20T20:30:25.644464"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "jnt_df.sort('n', ascending=).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-21T08:44:09.448645",
     "start_time": "2017-10-21T08:44:09.348414"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Employees_df = sqlCtx.read.format('csv').load('/user/pmolnar/data/AdventureWorks/Employees.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adventure Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-21T08:47:50.153485",
     "start_time": "2017-10-21T08:47:41.105191"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load adventureworks_spark.py\n",
    "if not 'sqlCtx' in vars():\n",
    "    sqlCtx = SQLContext(sc)\n",
    "\n",
    "Employees_df = sqlCtx.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header=True, inferschema=True)\\\n",
    "    .load('/user/pmolnar/data/AdventureWorks/Employees.csv.gz')\n",
    "\n",
    "Territory_df = sqlCtx.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header=True, inferschema=True)\\\n",
    "    .load('/user/pmolnar/data/AdventureWorks/SalesTerritory.csv.gz')\n",
    "\n",
    "Orders_df = sqlCtx.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header=True, inferschema=True)\\\n",
    "    .load('/user/pmolnar/data/AdventureWorks/ItemsOrdered.csv.gz')\n",
    "\n",
    "Customers_df = sqlCtx.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header=True, inferschema=True)\\\n",
    "    .load('/user/pmolnar/data/AdventureWorks/Customer.csv.gz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-21T08:47:57.382630",
     "start_time": "2017-10-21T08:47:57.368159"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  },
  "toc": {
   "nav_menu": {
    "height": "85px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
