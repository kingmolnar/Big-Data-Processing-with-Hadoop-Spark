{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Yelp Reviews\n",
    "Let's explore the Yelp reviews and perform a sentiment analysis:\n",
    "\n",
    "1. Load reviews from data file ... there are in JSON format\n",
    "2. Convert JSON records to Python tuples for earch row, extract only what we need\n",
    "3. Maybe Look at stars rating\n",
    "4. Create list of words (or bag-of-words)\n",
    "4. Load sentiment dictionary file and convert into a useful format.\n",
    "5. Assign sentiment values (pos and neg) to words of reviews\n",
    "6. Aggregate over reviews and report sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T10:47:03.738270",
     "start_time": "2017-10-20T10:47:03.696415"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load pyspark_init_arc.py\n",
    "#\n",
    "# This configuration works for Spark on macOS using homebrew\n",
    "#\n",
    "import os, sys\n",
    "# set OS environment variable\n",
    "os.environ[\"SPARK_HOME\"] = '/usr/hdp/2.4.2.0-258/spark'\n",
    "# add Spark library to Python\n",
    "sys.path.insert(0, os.path.join(os.environ[\"SPARK_HOME\"], 'python'))\n",
    "\n",
    "# import package\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext, SparkConf\n",
    "\n",
    "import atexit\n",
    "def stop_my_spark():\n",
    "    sc.stop()\n",
    "    del(sc)\n",
    "\n",
    "# Register exit    \n",
    "atexit.register(stop_my_spark)\n",
    "\n",
    "# Configure and start Spark ... but only once.\n",
    "if not 'sc' in globals():\n",
    "    conf = SparkConf()\n",
    "    conf.setAppName('MyFirstSpark') ## you may want to change this\n",
    "    conf.setMaster('yelp-client')\n",
    "    sc = SparkContext(conf=conf)\n",
    "    print \"Launched Spark version %s with ID %s\" % (sc.version, sc.applicationId)\n",
    "    print \"http://arc.insight.gsu.edu:8088/cluster/app/%s\"% (sc.applicationId)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T10:47:26.028213",
     "start_time": "2017-10-20T10:47:26.008575"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"http://arc.insight.gsu.edu:8088/cluster/app/%s\"% (sc.applicationId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T10:47:42.532644",
     "start_time": "2017-10-20T10:47:36.870015"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls /data/yelp/review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T10:47:54.568493",
     "start_time": "2017-10-20T10:47:54.563758"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATADIR='/data/yelp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T10:47:57.802694",
     "start_time": "2017-10-20T10:47:56.796796"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "review_rdd = sc.textFile(os.path.join(DATADIR, 'review/review_ab.json.gz')).sample(False, 0.01, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Glance at Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T10:48:06.956544",
     "start_time": "2017-10-20T10:48:02.915287"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "review_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-11T11:28:32.709403",
     "start_time": "2017-02-11T11:28:32.706532"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-11T11:20:45.629821",
     "start_time": "2017-02-11T11:20:39.731053"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how many elements do we actually have?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T10:49:53.815773",
     "start_time": "2017-10-20T10:49:53.812385"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text =  \"Mr Hoagie is an institution. Walking in, it does seem like a throwback to 30 years ago, old fashioned menu board, booths out of the 70s, and a large selection of food. Their speciality is the Italian Hoagie, and it is voted the best in the area year after year. I usually order the burger, while the patties are obviously cooked from frozen, all of the other ingredients are very fresh. Overall, its a good alternative to Subway, which is down the road.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T10:49:49.669606",
     "start_time": "2017-10-20T10:49:49.645919"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text2words(text):\n",
    "    import re\n",
    "    def clean_text(text):\n",
    "        return re.sub(r'[.;:,!\\'\"]', ' ', unicode(text).lower())\n",
    "    return filter(lambda x: x!='', clean_text(text).split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T10:50:03.322325",
     "start_time": "2017-10-20T10:50:03.316979"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text2words(text)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T11:46:35.715512",
     "start_time": "2017-10-20T11:46:35.710971"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def json_review(s):\n",
    "    import json\n",
    "    r = json.loads(s.strip())\n",
    "    return (r['business_id']+'^'+r['review_id'], r['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T11:46:37.915444",
     "start_time": "2017-10-20T11:46:37.805759"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# OK, let's create a `review_rdd` with only the elements we care bout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T11:46:50.539169",
     "start_time": "2017-10-20T11:46:50.535146"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# next thing create a word list `words_rdd`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T11:46:52.491668",
     "start_time": "2017-10-20T11:46:52.413695"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T10:53:58.513404",
     "start_time": "2017-10-20T10:53:52.584639"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls /data/yelp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T10:55:14.751886",
     "start_time": "2017-10-20T10:55:14.285396"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentidict = sc.textFile(os.path.join(DATADIR, 'SentiWordNet_3.0.0_20130122.txt'))\n",
    "print sentidict.count()\n",
    "sentidict.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T11:04:28.458625",
     "start_time": "2017-10-20T11:04:28.454671"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_senti_words(x):\n",
    "    xl = x.split(' ')\n",
    "    return map(lambda r: r.split('#')[0], xl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T11:04:30.235045",
     "start_time": "2017-10-20T11:04:30.230184"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_senti_words(u'dorsal#2 abaxial#1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T11:16:05.628664",
     "start_time": "2017-10-20T11:16:05.620706"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proc_senti_recs(s):\n",
    "    try:\n",
    "        sl = s.split('\\t')\n",
    "        pos = sl[0]\n",
    "        wid = sl[1]\n",
    "        pos = sl[2]\n",
    "        neg = sl[3]\n",
    "        wrds = split_senti_words(sl[4])\n",
    "        return [(w, (float(pos), float(neg))) for w in wrds]\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T11:16:07.826476",
     "start_time": "2017-10-20T11:16:07.733826"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now let's create a dictionary that we can join to the word list `sdict_rdd`\n",
    "# What are we going to do with the synonyms in the dictionary?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T11:47:16.560179",
     "start_time": "2017-10-20T11:47:13.525201"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's join and create a `jnt_rdd`\n",
    "# Hint: the joining key is the first element in the tuple\n",
    "\n",
    "\n",
    "\n",
    "jnt_rdd.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T11:47:34.239596",
     "start_time": "2017-10-20T11:47:32.401602"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# OK, after joining add up the positive and negative scores, create `res_rdd`\n",
    "# Hint: use reduceByKey\n",
    "\n",
    "\n",
    "res_rdd.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Statistics\n",
    "\n",
    "Using short cut calculation:\n",
    "\n",
    "$$mean = \\frac{1}{n} \\sum_{i=1}^n x_i$$\n",
    "\n",
    "$$var = \\frac{1}{n-1}( \\sum_{i=1}^n x_i^2 - \\frac{ (\\sum_{i=1}^n x_i)^2}{n})$$\n",
    "\n",
    "\n",
    "Suggestion https://stackoverflow.com/questions/39981312/spark-rdd-how-to-calculate-statistics-most-efficiently\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try reduceByKey. It's pretty straightforward if we only want to compute the min():\n",
    "\n",
    "    rdd.reduceByKey(lambda x,y: min(x,y)).collect()\n",
    "#Out[84]: [('key3', 2.0), ('key2', 3.0), ('key1', 1.0)]\n",
    "To calculate the mean, you'll first need to create (value, 1) tuples which we use to calculate both the sum and count in the reduceByKey operation. Lastly we divide them by each other to arrive at the mean:\n",
    "\n",
    "    meanRDD = (rdd\n",
    "               .mapValues(lambda x: (x, 1))\n",
    "               .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\n",
    "               .mapValues(lambda x: x[0]/x[1]))\n",
    "\n",
    "    meanRDD.collect()\n",
    "#Out[85]: [('key3', 5.5), ('key2', 5.0), ('key1', 3.3333333333333335)]\n",
    "For the variance, you can use the formula (sumOfSquares/count) - (sum/count)^2, which we translate in the following way:\n",
    "\n",
    "    varRDD = (rdd\n",
    "              .mapValues(lambda x: (1, x, x*x))\n",
    "              .reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1], x[2]+y[2]))\n",
    "              .mapValues(lambda x: (x[2]/x[0] - (x[1]/x[0])**2)))\n",
    "\n",
    "    varRDD.collect()\n",
    "#Out[106]: [('key3', 12.25), ('key2', 4.0), ('key1', 2.8888888888888875)]\n",
    "I used values of type double instead of int in the dummy data to accurately illustrate computing the average and variance:\n",
    "\n",
    "    rdd = sc.parallelize([(\"key1\", 1.0),\n",
    "                          (\"key3\", 9.0),\n",
    "                          (\"key2\", 3.0),\n",
    "                          (\"key1\", 4.0),\n",
    "                          (\"key1\", 5.0),\n",
    "                          (\"key3\", 2.0),\n",
    "                          (\"key2\", 7.0)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T12:28:08.301949",
     "start_time": "2017-10-20T12:28:08.190580"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We need to create a dataset to produce N=number of samples, mean(pos), var(pos), mean(neg), var(neg)\n",
    "# once we have that in place we can aggregate\n",
    "# Note: we are instested in mean and var per business...\n",
    "\n",
    "\n",
    "res2_rdd.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T12:03:57.081986",
     "start_time": "2017-10-20T12:03:56.865289"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now aggreate \n",
    "res2_rdd.reduceByKey(lambda a,b: (a[0]+b[0], a[1]+b[1], a[2]+b[2], a[3]+b[4], a[1]+b[4])).take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
